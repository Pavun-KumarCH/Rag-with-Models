{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOXGKYzVR4NuEypqX6GtbUs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavun-KumarCH/Research-Notebooks/blob/main/Anomaly_Detection_VDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection"
      ],
      "metadata": {
        "id": "JGiqiioweIf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title requirements\n",
        "%pip install --q pinecone sentence-transformers"
      ],
      "metadata": {
        "id": "QelNKXzReF00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evddbcG-d7AZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer, InputExample, models, util, losses\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Environments\n",
        "from google.colab import userdata\n",
        "PINECONE_API_KEY = userdata.get(\"PINECONE_API_KEY\")\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Pinecone\n",
        "pinecone = Pinecone(api_key = PINECONE_API_KEY)\n",
        "INDEX_NAME = \"ad-ai\"\n",
        "\n",
        "# Delete if already exsist\n",
        "if INDEX_NAME in pinecone.list_indexes():\n",
        "  pinecone.delete_index(INDEX_NAME)\n",
        "\n",
        "# Create index\n",
        "pinecone.create_index(\n",
        "    name = INDEX_NAME,\n",
        "    dimension = 256,\n",
        "    spec = ServerlessSpec(cloud = 'aws', region = 'us-east-1'),\n",
        ")\n",
        "index = pinecone.Index(INDEX_NAME)"
      ],
      "metadata": {
        "id": "yigXnBXMfL69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "!wget -q --show-progress -O training.tar.zip \"https://www.dropbox.com/scl/fi/rihfngx4ju5pzjzjj7u9z/lesson6.tar.zip?rlkey=rct9a9bo8euqgshrk8wiq2orh&dl=1\"\n",
        "\n",
        "!tar -xzvf training.tar.zip\n",
        "\n",
        "!tar -xzvf lesson6.tar"
      ],
      "metadata": {
        "id": "g65zqXYRgCkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -5 sample.log"
      ],
      "metadata": {
        "id": "tSx9qUm8hZd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -5 training.txt"
      ],
      "metadata": {
        "id": "XSRq55TIgTcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check cuda and Setup the Model\n",
        "We are using bert-base-uncased sentence-transformers model that maps sentences to a 256 dimensional dense vector space."
      ],
      "metadata": {
        "id": "FMpW3n4hgXnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length= 786)\n",
        "\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "\n",
        "dense_model = models.Dense(in_features = pooling_model.get_sentence_embedding_dimension(), out_features = 256,\n",
        "                           activation_function = nn.Tanh())\n",
        "\n",
        "model = SentenceTransformer(modules = [word_embedding_model, pooling_model, dense_model], device = device)\n",
        "\n",
        "device"
      ],
      "metadata": {
        "id": "M_6iOhhTgV5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the Model"
      ],
      "metadata": {
        "id": "_sl1jzl9hd0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = []\n",
        "with open('./training.txt', 'r') as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "      a, b, label = line.split('^')\n",
        "      train_examples.append(InputExample(texts = [a, b], label = float(label)))\n",
        "\n",
        "# Define dataset, the dataloader and the training loss\n",
        "warmups_steps = 100\n",
        "train_dataloader = DataLoader(train_examples, shuffle = True, batch_size = 16)\n",
        "train_loss = losses.CosineSimilarityLoss(model)"
      ],
      "metadata": {
        "id": "8S9WFXH7hfGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> (Note: load_pretrained_model = True): We've saved the trained model and are loading it here for speedier results, allowing you to observe the outcomes faster. Once you've done an initial run, you may set load_pretrained_model to False to train the model yourself. This can take some time to finsih, depending the value you set for the epochs.\n",
        "\n"
      ],
      "metadata": {
        "id": "MjyonN7fie-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import L\n",
        "import pickle\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "if load_pretrained_model:\n",
        "  trained_model_file = open(\"./data/pretrained_model\", \"rb\")\n",
        "  db = pickle.load(trained_model_file)\n",
        "  trained_model_file.close()\n",
        "else:\n",
        "  model.fit(train_objectives = [(train_dataloader, train_loss)], epochs = 16, warmup_steps = 100)\n",
        "\n",
        "samples = []\n",
        "with open('sample.log', 'r') as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "      samples.append(line)"
      ],
      "metadata": {
        "id": "e6BBtvY6igfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}